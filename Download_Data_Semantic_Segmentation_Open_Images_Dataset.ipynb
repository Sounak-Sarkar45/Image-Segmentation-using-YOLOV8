{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Computer Vision Engineer\n",
        "#\n",
        "# This project incorporates components from the Apache 2.0 licensed project.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "# ******************************************************************************\n",
        "# DISCLAIMER:\n",
        "#\n",
        "# This script is designed to download images and annotations from the Google Images\n",
        "# Dataset V7. It is important to note that the images and annotations in the\n",
        "# Google Images Dataset V7 are subject to specific licenses and usage terms. Users\n",
        "# of this script are strongly advised to refer to the Google Open Images website\n",
        "# (https://storage.googleapis.com/openimages/web/index.html) to verify and comply\n",
        "# with the licensing terms associated with both the images and annotations that\n",
        "# will be downloaded using this script.\n",
        "#\n",
        "# By using this script, you acknowledge and agree to adhere to the terms and\n",
        "# conditions set forth by the creators of the Google Images Dataset V7 for the\n",
        "# usage of both images and annotations. Any unauthorized use or violation of the\n",
        "# licensing terms is the sole responsibility of the user.\n",
        "# ******************************************************************************"
      ],
      "metadata": {
        "id": "9ePWOFXiLEqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install requirements\n",
        "\n",
        "!pip install boto3==1.33.3\n",
        "!pip install tqdm==4.66.1\n",
        "!pip install requests==2.31.0\n",
        "!pip install pandas==2.1.3\n",
        "!pip install opencv-python==4.8.1.78"
      ],
      "metadata": {
        "id": "-BR2GUuJh1Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hMExsxheZmeR"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "import os\n",
        "import shutil\n",
        "import argparse\n",
        "import zipfile\n",
        "import time\n",
        "\n",
        "import requests\n",
        "from requests.adapters import Retry\n",
        "\n",
        "import pandas as pd\n",
        "import cv2\n",
        "\n",
        "\n",
        "def mask_to_polygon(image_path, class_id):\n",
        "    mask = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    _, mask = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    H, W = mask.shape\n",
        "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # convert the contours to polygons\n",
        "    polygons = []\n",
        "    for cnt in contours:\n",
        "        if cv2.contourArea(cnt) > 200:\n",
        "            polygon = []\n",
        "            for point in cnt:\n",
        "                x, y = point[0]\n",
        "                polygon.append(x / W)\n",
        "                polygon.append(y / H)\n",
        "            polygons.append(polygon)\n",
        "\n",
        "    ret = ''\n",
        "\n",
        "    for polygon in polygons:\n",
        "        for p_, p in enumerate(polygon):\n",
        "            if p_ == len(polygon) - 1:\n",
        "                ret = ret + '{}\\n'.format(p)\n",
        "            elif p_ == 0:\n",
        "                ret = ret + str(class_id) + ' {} '.format(p)\n",
        "            else:\n",
        "                ret = ret + '{} '.format(p)\n",
        "    return ret\n",
        "\n",
        "\n",
        "def process(classes, data_out_dir):\n",
        "\n",
        "    train_mask_data_url = 'https://storage.googleapis.com/openimages/v5/train-annotations-object-segmentation.csv'\n",
        "    val_mask_data_url = 'https://storage.googleapis.com/openimages/v5/validation-annotations-object-segmentation.csv'\n",
        "    test_mask_data_url = 'https://storage.googleapis.com/openimages/v5/test-annotations-object-segmentation.csv'\n",
        "\n",
        "    downloader_url = 'https://raw.githubusercontent.com/openimages/dataset/master/downloader.py'\n",
        "    class_names_all_url = 'https://storage.googleapis.com/openimages/v7/oidv7-class-descriptions.csv'\n",
        "    class_ids_sem_seg_url = 'https://storage.googleapis.com/openimages/v7/oidv7-classes-segmentation.txt'\n",
        "\n",
        "    for url in [train_mask_data_url, val_mask_data_url, test_mask_data_url, class_ids_sem_seg_url, class_names_all_url,\n",
        "                downloader_url]:\n",
        "        if not os.path.exists(url.split('/')[-1]):\n",
        "\n",
        "            print('downloading {}...'.format(url.split('/')[-1]))\n",
        "\n",
        "            r = requests.get(url)\n",
        "            with open(url.split('/')[-1], 'wb') as f:\n",
        "                f.write(r.content)\n",
        "\n",
        "    class_ids = []\n",
        "\n",
        "    classes_all = pd.read_csv(class_names_all_url.split('/')[-1])\n",
        "\n",
        "    with open(class_ids_sem_seg_url.split('/')[-1], 'r') as f:\n",
        "        classes_sem_seg = [l[:-1] for l in f.readlines() if len(l) > 1]\n",
        "        f.close()\n",
        "\n",
        "    for class_ in classes:\n",
        "        if class_ not in list(classes_all['DisplayName']):\n",
        "            raise Exception('Class name not found: {}'.format(class_))\n",
        "        class_index = list(classes_all['DisplayName']).index(class_)\n",
        "        class_id_ = classes_all['LabelName'].iloc[class_index]\n",
        "        if class_id_ in classes_sem_seg:\n",
        "            class_ids.append(class_id_)\n",
        "        else:\n",
        "            raise Exception('Class name not found: {}'.format(class_))\n",
        "\n",
        "\n",
        "    image_list_file_path = os.path.join('.', 'image_list_file')\n",
        "    if os.path.exists(image_list_file_path):\n",
        "        os.remove(image_list_file_path)\n",
        "\n",
        "    image_list_file_list = []\n",
        "    mask_paths = []\n",
        "    for j, url in enumerate([train_mask_data_url, val_mask_data_url, test_mask_data_url]):\n",
        "        filename = url.split('/')[-1]\n",
        "        with open(filename, 'r') as f:\n",
        "            line = f.readline()\n",
        "            while len(line) != 0:\n",
        "                mask_path, id, class_name, _, _, _, _, _, _, _ = line.split(',')[:13]\n",
        "                if class_name in class_ids:\n",
        "                    mask_paths.append(['train', 'validation', 'test'][j] + '/' + mask_path)\n",
        "                    if id not in image_list_file_list:\n",
        "                        image_list_file_list.append(id)\n",
        "                        with open(image_list_file_path, 'a') as fw:\n",
        "                            fw.write('{}/{}\\n'.format(['train', 'validation', 'test'][j], id))\n",
        "                            fw.close()\n",
        "                line = f.readline()\n",
        "\n",
        "            f.close()\n",
        "\n",
        "    out_dir = './.out'\n",
        "    shutil.rmtree(out_dir, ignore_errors=True)\n",
        "    os.makedirs(out_dir)\n",
        "    # os.system('python downloader.py {} --download_folder={}'.format(image_list_file_path, out_dir))\n",
        "    !python downloader.py ./image_list_file --download_folder=./.out\n",
        "\n",
        "    # download all masks\n",
        "    out_masks_dir_ = './.out_masks_all'\n",
        "    shutil.rmtree(out_masks_dir_, ignore_errors=True)\n",
        "    os.makedirs(out_masks_dir_)\n",
        "    for set_ in ['train', 'validation', 'test']:\n",
        "        dir_ = os.path.join(out_masks_dir_, set_)\n",
        "        # if os.path.exists(dir_):\n",
        "        #     shutil.rmtree(dir_)\n",
        "        os.makedirs(dir_, exist_ok=True)\n",
        "        for k in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                  'a', 'b', 'c', 'd', 'e', 'f']:\n",
        "            url = 'https://storage.googleapis.com/openimages/v5/{}-masks/{}-masks-{}.zip'.format(set_, set_, k)\n",
        "\n",
        "            if not os.path.exists('{}-masks-{}.zip'.format(set_, k)):\n",
        "\n",
        "                print('downloading {}...'.format('{}-masks-{}.zip'.format(set_, k)))\n",
        "\n",
        "                for retry in range(1, 10):\n",
        "                    retry_because_of_timeout = False\n",
        "                    try:\n",
        "                        r = requests.get(url, timeout=5)\n",
        "                    except Exception as e:\n",
        "                        retry_because_of_timeout = True\n",
        "                        print(e)\n",
        "\n",
        "                    if retry_because_of_timeout:\n",
        "                        print('retry', retry, url.split('/')[-1])\n",
        "                        time.sleep(retry * 2 + 1)\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                with open(url.split('/')[-1], 'wb') as f:\n",
        "                    f.write(r.content)\n",
        "\n",
        "            with zipfile.ZipFile(url.split('/')[-1], 'r') as zip_ref:\n",
        "                zip_ref.extractall(dir_ + '/')\n",
        "\n",
        "            os.remove(url.split('/')[-1])\n",
        "\n",
        "            for img_path_ in os.listdir(dir_):\n",
        "                if '{}/{}'.format(set_, img_path_) not in mask_paths:\n",
        "                    os.remove(os.path.join(dir_, img_path_))\n",
        "\n",
        "    for set_ in ['train', 'validation', 'test']:\n",
        "        for dir_ in [os.path.join(data_out_dir, 'images', set_),\n",
        "                     os.path.join(data_out_dir, 'labels', set_)]:\n",
        "            if os.path.exists(dir_):\n",
        "                shutil.rmtree(dir_)\n",
        "            os.makedirs(dir_)\n",
        "\n",
        "    for mask_path in mask_paths:\n",
        "        set_ = mask_path.split(os.sep)[0]\n",
        "        image_id = mask_path.split(os.sep)[1][:-4][:16]\n",
        "        label_name = mask_path.split(os.sep)[1][:-4][17:-9]\n",
        "        print(mask_path, set_, image_id, label_name, os.path.exists(os.path.join(out_dir, '{}.jpg'.format(image_id))))\n",
        "        if os.path.exists(os.path.join(out_dir, '{}.jpg'.format(image_id))):\n",
        "            shutil.move(os.path.join(out_dir, '{}.jpg'.format(image_id)),\n",
        "                        os.path.join(data_out_dir, 'images', set_, '{}.jpg'.format(image_id)))\n",
        "\n",
        "        if os.path.exists(os.path.join(data_out_dir, 'images', set_, '{}.jpg'.format(image_id))):\n",
        "            with open(os.path.join(data_out_dir, 'labels', set_, '{}.txt'.format(image_id)), 'a') as f:\n",
        "                f.write('{}'.format(mask_to_polygon(os.path.join(out_masks_dir_, set_, mask_path.split(os.sep)[1]),\n",
        "                                                    int([c.replace('/', '') for c in class_ids].index(label_name)))))\n",
        "                f.close()\n",
        "\n",
        "    shutil.rmtree(out_dir, ignore_errors=True)\n",
        "    shutil.rmtree(out_masks_dir_, ignore_errors=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['Duck']  # list containing all the classes you will download from the open images dataset v7\n",
        "\n",
        "out_dir = './data'\n",
        "\n",
        "process(classes, out_dir)"
      ],
      "metadata": {
        "id": "RVWBpFLxeQsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zip the data directory\n",
        "\n",
        "!zip -r data.zip /content/data"
      ],
      "metadata": {
        "id": "z5cAay6Sd7p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBXbyXXCeFkQ",
        "outputId": "d7870c09-32a1-460f-9875-0159205799bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# copy data to your Google Drive\n",
        "\n",
        "!scp '/content/data.zip' '/content/gdrive/My Drive/data.zip'"
      ],
      "metadata": {
        "id": "PQWt6zgveKE7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}